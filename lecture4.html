<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistics: Data analysis and modelling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Maarten Speekenbrink" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mycss.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Statistics: Data analysis and modelling
]
.subtitle[
## Week 4: ANCOVA and linear mixed-effects models
]
.author[
### Maarten Speekenbrink
]

---











## The General Linear Model (GLM)

The General Linear Model has the form

`$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$`

`\(Y\)` is the **dependent variable**

`\(X_j\)` is a **predictor** or **independent variable**

`\(\beta_0\)` is called the **intercept**

`\(\beta_j\)` `\((j &gt; 0)\)` is called a **slope**

`\(\epsilon_i\)` is called the **error** or **residual**

---

## ANCOVA

`$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$`

* In ANalysis of COVAriance (ANCOVA), the predictors `\(X_j\)` are a mix of contrast-coded and measured variables
--

* Measured variables (covariates) are often included to control for pre-existing differences between groups.
--

* A consideration is whether the effect of covariates is allowed to differ between groups (an interaction between measured and contrast-coded variables). __Homogeneity of regression slopes__: Covariates are assumed to have the *same* effect in all groups.
--

* Group differences may be difficult to interpret when effect of covariates differs between groups.

---

## Feeling of power



&lt;img src="lecture4_files/figure-html/unnamed-chunk-4-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## ANOVA


``` r
contrasts(dat$primeCond)
```

```
   [,1]
PL -0.5
PH  0.5
```

``` r
contrasts(dat$expBelief)
```

```
   [,1]
EL -0.5
EH  0.5
```

---

## ANOVA


``` r
mod &lt;- lm(powerPOST ~ primeCond*expBelief, data=dat)
summary(mod)
```

```

Call:
lm(formula = powerPOST ~ primeCond * expBelief, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-42.998 -11.004   0.397  12.253  36.902 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           57.31509    0.77769  73.699   &lt;2e-16 ***
primeCond1             3.87484    1.55538   2.491   0.0131 *  
expBelief1            -0.01016    1.55538  -0.007   0.9948    
primeCond1:expBelief1  3.80235    3.11076   1.222   0.2223    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15.55 on 396 degrees of freedom
Multiple R-squared:  0.01915,	Adjusted R-squared:  0.01172 
F-statistic: 2.577 on 3 and 396 DF,  p-value: 0.05344
```

---

## ANCOVA with homogenous regression slopes




``` r
mod &lt;- lm(powerPOST ~ primeCond*expBelief + powerPRE, data=dat)
summary(mod)
```

```

Call:
lm(formula = powerPOST ~ primeCond * expBelief + powerPRE, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-41.943  -6.238   1.062   7.518  35.963 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           16.07075    2.18986   7.339 1.24e-12 ***
primeCond1             4.37877    1.11270   3.935 9.82e-05 ***
expBelief1            -0.07860    1.11240  -0.071    0.944    
powerPRE               0.71996    0.03697  19.473  &lt; 2e-16 ***
primeCond1:expBelief1  1.33357    2.22840   0.598    0.550    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.12 on 395 degrees of freedom
Multiple R-squared:  0.4996,	Adjusted R-squared:  0.4945 
F-statistic: 98.58 on 4 and 395 DF,  p-value: &lt; 2.2e-16
```

---

## Homogenous regression slopes

&lt;img src="lecture4_files/figure-html/unnamed-chunk-9-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## ANCOVA with homogeneous regression slopes

* Covariate(s) allow controlling for pre-existing differences. This can increase power of tests of contrasts.
* Effects of contrast codes are *unique* effects which are not accounted for by covariates.
* As usual, intercept is predicted value when all predictors (including covariates) equal 0. Even with sum-to-0 contrasts, this is not the grand mean.

---

## Center covariate

.scrollable[

``` r
dat$powerPREc &lt;- as.numeric(center(dat$powerPRE))
mod1 &lt;- lm(powerPOST ~ primeCond*expBelief + powerPREc, data=dat)
summary(mod1)
```

```

Call:
lm(formula = powerPOST ~ primeCond * expBelief + powerPREc, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-41.943  -6.238   1.062   7.518  35.963 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           57.31400    0.55620 103.046  &lt; 2e-16 ***
primeCond1             4.37877    1.11270   3.935 9.82e-05 ***
expBelief1            -0.07860    1.11240  -0.071    0.944    
powerPREc              0.71996    0.03697  19.473  &lt; 2e-16 ***
primeCond1:expBelief1  1.33357    2.22840   0.598    0.550    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.12 on 395 degrees of freedom
Multiple R-squared:  0.4996,	Adjusted R-squared:  0.4945 
F-statistic: 98.58 on 4 and 395 DF,  p-value: &lt; 2.2e-16
```
]
---

## Non-homogeneous regresssion slopes


.scrollable[

``` r
mod2 &lt;- lm(powerPOST ~ primeCond*expBelief*powerPREc, data=dat)
summary(mod2)
```

```

Call:
lm(formula = powerPOST ~ primeCond * expBelief * powerPREc, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-40.322  -6.205   1.089   7.323  33.675 

Coefficients:
                                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                     57.28624    0.55857 102.559  &lt; 2e-16 ***
primeCond1                       4.33683    1.11714   3.882 0.000121 ***
expBelief1                      -0.03189    1.11714  -0.029 0.977243    
powerPREc                        0.72167    0.03716  19.419  &lt; 2e-16 ***
primeCond1:expBelief1            1.36434    2.23427   0.611 0.541790    
primeCond1:powerPREc            -0.03528    0.07433  -0.475 0.635263    
expBelief1:powerPREc             0.04773    0.07433   0.642 0.521125    
primeCond1:expBelief1:powerPREc  0.09541    0.14865   0.642 0.521341    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 11.15 on 392 degrees of freedom
Multiple R-squared:  0.501,	Adjusted R-squared:  0.4921 
F-statistic: 56.22 on 7 and 392 DF,  p-value: &lt; 2.2e-16
```
]
---

## Non-homogeneous regresssion slopes

&lt;img src="lecture4_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Testing homogeneity of regression slopes


``` r
anova(mod1, mod2)
```

```
Analysis of Variance Table

Model 1: powerPOST ~ primeCond * expBelief + powerPREc
Model 2: powerPOST ~ primeCond * expBelief * powerPREc
  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)
1    395 48876                           
2    392 48736  3     139.8 0.3748 0.7712
```

---

class: center, inverse


# Linear mixed-effects models

---

## Assumptions of the GLM

`$$Y_i = \beta_0 + \beta_1 \times X_{1,i} + \beta_2 \times X_{2,i} + \ldots + \beta_m \times X_{m,i} + \epsilon_i \quad \quad \epsilon_i \sim \mathbf{Normal}(0,\sigma_\epsilon)$$`

1. __Normality__: the errors `\(\epsilon_i\)` are Normal-distributed
2. __Unbiasedness__: the mean of `\(\epsilon_i\)` is 0.
3. __Homoscedasticity__: the errors `\(\epsilon_i\)` have a constant variance `\(\sigma^2_\epsilon\)`
4. __Independence__: any error term `\(\epsilon_i\)` is independent of any other `\(\epsilon_j\)` (for all `\(i\)` and `\(j \neq i\)`).

--

If subsets of observations come from the same __unit of observation__ (e.g. a participant), the assumption of independence may be violated.

--

`$$Y_{i,j} = \beta_0 + \beta_1 \times X_{1,i,j} + \beta_2 \times X_{2,i,j} + \ldots + \beta_m \times X_{m,i,j} + \epsilon_{i,j} \quad \quad \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_\epsilon)$$`
where `\(i = 1, \ldots, n_j\)` denotes the `\(i\)`-th observation for unit/grouping `\(j = 1, \ldots, n_g\)`.

---

## Linear mixed-effects models

`$$\begin{aligned} Y_{i,k} &amp;= (\beta_0 + \gamma_{0,k}) + (\beta_1 + \gamma_{1,k}) \times X_{1,i,k} + \ldots + (\beta_m + \gamma_{m,k}) \times X_{m,i,k} + \epsilon_{i,k} \\
&amp; \epsilon_{i,k} \sim \mathbf{Normal}(0,\sigma_{\epsilon}) \\
&amp; \gamma_{j,k} \sim \mathbf{Normal}(\boldsymbol{0}, \boldsymbol{\Sigma}_\gamma) \end{aligned}$$`

* `\(Y_{i,k}\)` is observation `\(i\)` within **unit of observation** `\(k\)` `\((i=1,\ldots, n_k)\)`
* `\(\beta_j\)` is a __fixed effect__ `\((j=0, \ldots, m)\)`
* `\(\gamma_{j,k}\)` is a __random effect__ for parameter `\(j\)` and unit of observation `\(k\)`.
* `\(\boldsymbol{\Sigma}_\gamma\)` is the __covariance matrix__ of the random effects.
--


Key intuitions: 

* A linear-mixed effects model allows for what is effectively a different regression equation for each unit of observation `\(j\)`.
* We need multiple observations `\(i\)` for each unit of observation `\(j\)` to estimate such a model.
* Rather than estimating separate models for each subset `\(j\)`, we can estimate a single model over all subsets, assuming the random effects are drawn from a multivariate Normal distribution.
* Each fixed effect `\(\beta_k\)` can be viewed as the average effect over all possible units of observation `\(j\)`.

---

## Random intercepts models

`$$\begin{aligned} Y_{i,j} &amp;= (\beta_0 + \gamma_{0,j}) + \beta_1 \times X_{1,i,j} + \ldots + \beta_m \times X_{m,i,j} + \epsilon_{i,j} \\
&amp; \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon}) \\
&amp; \gamma_{0,j} \sim \mathbf{Normal}(0, \sigma_{\gamma_0}) \end{aligned}$$`

--

Like ANCOVA (with homogeneity of regression slopes): the effect of predictors `\(X_j\)` is the same for each unit of observation.

---


## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

--

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`

--

&amp;nbsp;

`$$\gamma_{j,0} = \beta_{0,j} - \beta_0$$`

--

&amp;nbsp;

`$$\beta_{0,j} \sim \mathbf{Normal}(\beta_0,\sigma_{\gamma_0})$$`

---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`

&lt;img src="lecture4_files/figure-html/unnamed-chunk-15-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`

&lt;img src="lecture4_files/figure-html/unnamed-chunk-16-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`

&lt;img src="lecture4_files/figure-html/unnamed-chunk-17-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`

&lt;img src="lecture4_files/figure-html/unnamed-chunk-18-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`


&lt;img src="lecture4_files/figure-html/unnamed-chunk-19-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---

## A random intercepts model

`\begin{equation}
Y_{i,j} = \beta_{0,j} + \epsilon_{i,j} \quad \quad \quad  \epsilon_{i,j} \sim \mathbf{Normal}(0,\sigma_{\epsilon})
\end{equation}`

`\begin{equation}
\beta_{0,j} = \beta_0 + \gamma_{0,j} \quad \quad \quad \gamma_{0,j} \sim \mathbf{Normal}(0,\sigma_{\gamma_0})
\end{equation}`


&lt;img src="lecture4_files/figure-html/unnamed-chunk-20-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Random intercepts and anchoring

&lt;img src="lecture4_files/figure-html/unnamed-chunk-21-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Random intercepts and anchoring


``` r
modg &lt;- afex::mixed(everest_meters ~ 1 + (1|referrer), data=dat, 
                   test_intercept = TRUE)
modg
```

```
Mixed Model Anova Table (Type 3 tests, S-method)

Model: everest_meters ~ 1 + (1 | referrer)
Data: dat
[1] Effect  df      F       p.value
&lt;0 rows&gt; (or 0-length row.names)
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```

---

## Random intercepts and anchoring


``` r
summary(modg)
```

```
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: everest_meters ~ 1 + (1 | referrer)
   Data: data

REML criterion at convergence: 6961.1

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5288 -0.6822 -0.0781  0.6025  3.5153 

Random effects:
 Groups   Name        Variance Std.Dev.
 referrer (Intercept) 4942695  2223    
 Residual             6854543  2618    
Number of obs: 374, groups:  referrer, 9

Fixed effects:
            Estimate Std. Error       df t value Pr(&gt;|t|)    
(Intercept) 4392.911    769.653    7.213   5.708 0.000655 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---

## Restricted maximum likelihood (REML)

* Maximum likelihood estimates of variances `\(\sigma^2_\epsilon\)` and `\(\sigma^2_{\gamma}\)` are biased. This is because they are based on a estimated parameters (e.g. mean) which are noisy. E.g. the MLE
`$$\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n}$$`
vs unbiased estimate
`$$\hat{\sigma}^2_\epsilon = \frac{\sum_{i=1}^n (Y_i - \overline{Y})^2}{n-1}$$`
* Restricted maximum likelihood (REML) aims to provide estimates of variance components which are unbiased. 
* For large sample sizes, ML and REML give similar estimates, but for smaller sample sizes, the difference can be substantial


---

## Fixed and random effects


``` r
lme4::fixef(modg$full_model)
```

```
(Intercept) 
   4392.911 
```

``` r
lme4::ranef(modg$full_model)
```

```
$referrer
         (Intercept)
brasilia -1596.32519
charles   2513.92787
ku          91.98951
laurier  -2377.93774
msvu     -2834.49561
swps      -130.04071
swpson    2877.70772
tilburg   -799.05439
unipd     2254.22854

with conditional variances for "referrer" 
```

---

## Fixed and random effects

Estimated random intercepts: `\(\hat{\beta}_{0,j} = \hat{\beta}_0 + \hat{\gamma}_{0,j}\)`


``` r
lme4::fixef(modg$full_model) + lme4::ranef(modg$full_model)$referrer
```

```
         (Intercept)
brasilia    2796.586
charles     6906.839
ku          4484.901
laurier     2014.974
msvu        1558.416
swps        4262.871
swpson      7270.619
tilburg     3593.857
unipd       6647.140
```

---

## Shrinkage and partial pooling



`$$\begin{align}
Y_{i,j} &amp;= \beta_0 + \gamma_{0,j} + \epsilon_{i,j} \\
&amp;= 4392.91 + \gamma_{0,j} + \epsilon_{i,j} \\
\gamma_{0,j} &amp;\sim \mathbf{Normal}(0,2223.22) \\
\epsilon_{i,j} &amp;\sim \mathbf{Normal}(0,2618.12)
\end{align}$$`


|         | `\(\overline{Y}_j\)`| `\(n_j\)`| `\(\hat{\beta}_{0,j}\)`| `\(\hat{\gamma}_{0,j}\)`|
|:--------|----------------:|-----:|-------------------:|--------------------:|
|brasilia |          2746.27|    44|             2796.59|             -1596.33|
|charles  |          8650.00|     2|             6906.84|              2513.93|
|ku       |          4488.44|    36|             4484.90|                91.99|
|laurier  |          1946.27|    48|             2014.97|             -2377.94|
|msvu     |          1431.61|    31|             1558.42|             -2834.50|
|swps     |          4257.86|    36|             4262.87|              -130.04|
|swpson   |          7325.29|    73|             7270.62|              2877.71|
|tilburg  |          3570.77|    48|             3593.86|              -799.05|
|unipd    |          6702.96|    56|             6647.14|              2254.23|


---

## Shrinkage and partial pooling

&lt;img src="lecture4_files/figure-html/unnamed-chunk-28-1.png" width="90%" style="display: block; margin: auto;" /&gt;


---


``` r
data("anchoring")
dat &lt;- subset(anchoring, us_or_international == "International" &amp; referrer != "lse") |&gt;
  mutate(referrer = factor(referrer),
         anchor = factor(anchor))
modg &lt;- afex::mixed(everest_meters ~ 1 + anchor + (1|referrer), data=dat)
modg
```

```
Mixed Model Anova Table (Type 3 tests, S-method)

Model: everest_meters ~ 1 + anchor + (1 | referrer)
Data: dat
  Effect        df          F p.value
1 anchor 1, 645.62 456.35 ***   &lt;.001
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```

``` r
afex::mixed(everest_meters ~ 1 + anchor + (1|referrer), data=dat, method = "KR")
```

```
Mixed Model Anova Table (Type 3 tests, KR-method)

Model: everest_meters ~ 1 + anchor + (1 | referrer)
Data: dat
  Effect        df          F p.value
1 anchor 1, 645.52 455.54 ***   &lt;.001
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```


---


``` r
summary(modg)
```

```
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: everest_meters ~ 1 + anchor + (1 | referrer)
   Data: data

REML criterion at convergence: 12218.9

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.7440 -0.8062 -0.2369  1.0046  2.7917 

Random effects:
 Groups   Name        Variance Std.Dev.
 referrer (Intercept)  830415   911.3  
 Residual             8414294  2900.7  
Number of obs: 651, groups:  referrer, 9

Fixed effects:
            Estimate Std. Error      df t value Pr(&gt;|t|)    
(Intercept)  6869.87     334.48    7.95   20.54 3.57e-08 ***
anchor1      2481.14     116.15  645.62   21.36  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
        (Intr)
anchor1 0.051 
```

---

## Do we need random intercepts?


``` r
## estimaye model without random intercepts
modr &lt;- lm(everest_meters ~ 1 + anchor, data=dat)
logLik(modr)
```

```
'log Lik.' -6142.689 (df=3)
```

``` r
logLik(modg$full_model)
```

```
'log Lik.' -6109.433 (df=4)
```

---

## Do we need random intercepts?

`$$-2 \log (\text{likelihood-ratio}) = -2 \log p(\text{DATA}|\text{MODEL R}) - (-2 \log p(\text{DATA}|\text{MODEL G}))$$`


``` r
## likelihood ratio test
chisq &lt;- as.numeric(-2*logLik(modr) - -2*logLik(modg$full_model))
chisq
```

```
[1] 66.51199
```

``` r
1 - pchisq(chisq, df=1)
```

```
[1] 3.330669e-16
```

&lt;img src="lecture4_files/figure-html/unnamed-chunk-33-1.png" width="50%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
